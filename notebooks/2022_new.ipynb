{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import config as c\n",
    "from typing import NamedTuple, Optional\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from collections import defaultdict\n",
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileNames(NamedTuple):\n",
    "    \"\"\"class to store input filenames\"\"\"\n",
    "    main: str\n",
    "    informators: str = \"data/undone/tblInformators.csv\"\n",
    "    collectors: str = \"data/undone/tblSobirately.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoNames(NamedTuple):\n",
    "    \"class to store district and region\"\n",
    "    district: str\n",
    "    region: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTables(NamedTuple):\n",
    "    \"\"\"class to store geotables\"\"\"\n",
    "    villages: pd.DataFrame\n",
    "    regions: pd.DataFrame\n",
    "    districts: pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinedTable():\n",
    "    def __init__(self, tablename: str, engine: sqlalchemy.engine.Engine):\n",
    "        self.engine: sqlalchemy.engine.Engine = engine\n",
    "        self.tablename: str = tablename\n",
    "        self.dataframe: pd.DataFrame = pd.read_sql_table(\n",
    "            self.tablename,\n",
    "            schema=c.DB,\n",
    "            con=self.engine\n",
    "        )\n",
    "        if self.dataframe.shape[0] > 0:\n",
    "            self.last_index = self.dataframe[\"id\"].iloc[-1]\n",
    "        else:\n",
    "            self.last_index = 0\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return self.dataframe\n",
    "\n",
    "    def drop_duplicates(self, *args, **kwargs):\n",
    "        kwargs[\"inplace\"] = (\n",
    "            kwargs.get(\"inplace\") or False\n",
    "        )\n",
    "        kwargs[\"subset\"] = (\n",
    "            kwargs.get(\"subset\") \n",
    "            or [col for col in self.dataframe.columns if \"id\" not in col]\n",
    "        )\n",
    "        if not kwargs[\"inplace\"]:\n",
    "            self.dataframe = self.dataframe.drop_duplicates(*args, **kwargs)\n",
    "            return self\n",
    "        self.dataframe.drop_duplicates(*args, **kwargs)\n",
    "\n",
    "    def join(self, new_dataframe: pd.DataFrame):\n",
    "        if not \"id\" in new_dataframe.columns:\n",
    "            new_dataframe.insert(\n",
    "                loc=0,\n",
    "                column=\"id\",\n",
    "                value=list(\n",
    "                    range(self.last_index + 1, self.last_index + 1 + new_dataframe.shape[0]))\n",
    "                )\n",
    "        self.dataframe = self.dataframe.append(new_dataframe)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def new_indices(self) -> pd.Series:\n",
    "        return self.dataframe.loc[self.dataframe[\"id\"] > self.last_index][\"id\"]\n",
    "\n",
    "    @property\n",
    "    def new_entries(self) -> pd.DataFrame:\n",
    "        return self.dataframe.iloc[-self.new_indices.shape[0]:,:]\n",
    "\n",
    "    def upload(self):\n",
    "        if self.new_indices.shape[0] == 0:\n",
    "            print(f\"nothing to insert in {self.tablename}\")\n",
    "            return\n",
    "        self.new_entries.to_sql(\n",
    "            name=self.tablename,\n",
    "            con=self.engine,\n",
    "            schema=c.DB,\n",
    "            index=False,\n",
    "            if_exists=\"append\"\n",
    "        )\n",
    "        print(f\"upload to {self.tablename} complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinedRelation(JoinedTable):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tablename: str,\n",
    "        engine: sqlalchemy.engine.Engine,\n",
    "    ):\n",
    "        super().__init__(tablename, engine)\n",
    "        \n",
    "    def join(\n",
    "        self,\n",
    "        main_dataframe: pd.DataFrame,\n",
    "        relation_dataframe: pd.DataFrame,\n",
    "        on: tuple = (\"value\", \"code\"),\n",
    "        suffixes: tuple = (\"_x\", \"_y\")\n",
    "    ):\n",
    "        new_dataframe = pd.merge(\n",
    "            left=main_dataframe,\n",
    "            right=relation_dataframe,\n",
    "            suffixes=suffixes,\n",
    "            left_on=[on[0]],\n",
    "            right_on=[on[1]],\n",
    "            # how=\"left\"\n",
    "        )\n",
    "        id_cols = [\"id\" + suffixes[0], \"id\" + suffixes[1]]\n",
    "        new_dataframe = new_dataframe[id_cols]\n",
    "        # self.dataframe = self.dataframe.iloc[0:0]\n",
    "        return super().join(new_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_main_file(filename: str):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    else:\n",
    "        df = pd.read_excel(filename)\n",
    "    df.drop([\"zvuk2\", \"photo\", \"photo2\", \"термины\", \"аннотация\"], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "def read_sup_file(filename: str):\n",
    "    return pd.read_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_text_df(origin: pd.DataFrame, leader=\"АБМ\"):\n",
    "    \"\"\"Insert values from the origin df into a text df template\"\"\"\n",
    "    new_df = pd.DataFrame(columns=[\"old_id\", \"year\", \"leader\", \"address\", \"raw_text\", \"old_genre\", 'geo_id', \"genre\", \"pdf\"])\n",
    "    new_df[[\"raw_text\", \"year\"]] = origin[[\"текст\", \"год\"]]\n",
    "    new_df[\"leader\"] = leader\n",
    "    new_df = new_df.fillna(np.nan).replace([np.nan], [None])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_place_df(\n",
    "    main_file: pd.DataFrame,\n",
    "    inform_file: pd.DataFrame,\n",
    "    geonames: GeoNames = None\n",
    ") -> pd.DataFrame:\n",
    "    if not geonames:\n",
    "        raise ValueError(\"Names for district and region are missing\")\n",
    "    district, oblast = geonames\n",
    "    all_places: pd.Series = pd.concat([main_file[\"село\"], inform_file[\"село\"]])\n",
    "    all_places = all_places.str.strip().str.replace(\"  \", \" \")\n",
    "    vil_df = pd.DataFrame(columns=[\"village_name\", \"map_region\", \"map_district\"])\n",
    "    vil_df[\"village_name\"] = all_places.unique()\n",
    "    vil_df[\"map_region\"] = oblast\n",
    "    vil_df[\"map_district\"] = district\n",
    "    return vil_df\n",
    "\n",
    "def produce_place_df_complex(main_file: pd.DataFrame, inform_file: pd.DataFrame):\n",
    "    all_places: pd.Series = main_file[\"село\"]\n",
    "    all_places = all_places.str.strip().str.replace(\"  \", \" \")\n",
    "    vil_df: pd.DataFrame = all_places.str.split(\", \", expand=True)\n",
    "    vil_df.columns = [\"map_region\", \"map_district\", \"village_name\"]\n",
    "    vil_df[\"map_region\"] = vil_df[\"map_region\"].str.replace(\"обл.\", \"область\")\n",
    "    vil_df[\"map_district\"] = vil_df['map_district'].str.replace(\"р-н\", \"район\")\n",
    "    for idx, row in vil_df.iterrows():        \n",
    "        if row[\"village_name\"] is not None:\n",
    "            continue\n",
    "        row[[\"map_region\", \"village_name\"]] = row[[\"map_region\", \"map_district\"]]\n",
    "        row[\"map_district\"] = None\n",
    "        \n",
    "    vil_df = vil_df[[\"village_name\", \"map_region\", \"map_district\"]]\n",
    "    return vil_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_region_district(place_df: pd.DataFrame):\n",
    "    regions = place_df[\"map_region\"].dropna().apply(lambda x: x.strip().replace(\"  \", \" \")).unique()\n",
    "    districts = place_df[\"map_district\"].dropna().apply(lambda x: x.strip().replace(\"  \", \" \")).unique()\n",
    "    region_df = pd.DataFrame(regions, columns=[\"region_name\"])\n",
    "    district_df = pd.DataFrame(districts, columns=[\"district_name\"])\n",
    "    return GeoTables(villages=place_df, regions=region_df, districts=district_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_geo_df(\n",
    "    vil_df: pd.DataFrame,\n",
    "    reg_df: pd.DataFrame,\n",
    "    dis_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Use after the dfs have been concatenated with tables from the server\n",
    "    Produced table should be concatenated with g_geo_people && g_geo_text\n",
    "    \"\"\"\n",
    "    assert all(map(lambda x: \"id\" in x.columns, [vil_df, reg_df, dis_df]))\n",
    "    geo_df = pd.DataFrame(\n",
    "        columns=[\"id_region\", \"id_district\", \"id_village\"]\n",
    "    )\n",
    "    geo_df[\"id_village\"] = vil_df[\"id\"]\n",
    "\n",
    "    region_ids = pd.merge(\n",
    "        vil_df, reg_df, how='left', left_on=[\"map_region\"], right_on=[\"region_name\"], suffixes=(\"_village\", \"_region\")\n",
    "    )[\"id_region\"].dropna()\n",
    "    district_ids = pd.merge(\n",
    "        vil_df, dis_df, how='left', left_on=[\"map_district\"], right_on=[\"district_name\"], suffixes=(\"_village\", \"_district\")\n",
    "    )[\"id_district\"].dropna()\n",
    "\n",
    "    assert region_ids.shape[0] == vil_df.shape[0]\n",
    "    assert district_ids.shape[0] == vil_df.shape[0]\n",
    "\n",
    "    geo_df[\"id_region\"] = region_ids\n",
    "    geo_df[\"id_district\"] = district_ids\n",
    "    return geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_ids(main_file: pd.DataFrame, place_df: pd.DataFrame, geo_df: pd.DataFrame):\n",
    "    assert \"id\" in geo_df.columns, \"Geo df and geo table need to be joined first\"\n",
    "    main_file[\"село\"] = main_file[\"село\"].apply(lambda x: x.split(\", \")[-1].strip())\n",
    "    print(main_file.tail())\n",
    "    df = pd.merge(\n",
    "        main_file, place_df, how='left', left_on=[\"село\"], right_on=[\"village_name\"]\n",
    "    )\n",
    "    # print(*df.shape, sep=\", \")\n",
    "    df = pd.merge(\n",
    "        df, geo_df, how='left', left_on=[\"id\"], right_on=[\"id_village\"], suffixes=(\"_old\", \"_geo\")\n",
    "    )\n",
    "    # print(*df.shape, sep=\", \")\n",
    "    # print(df.shape[0], main_file.shape[0], sep=\" - new. Main: \")\n",
    "    assert df.shape[0] == main_file.shape[0]\n",
    "    return df[\"id_geo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(bio):\n",
    "    years = re.findall(\"([0-9]{4}) +г. *р.\", bio[:20])\n",
    "    if years:\n",
    "        return int(years[0])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_name(name):\n",
    "    global m\n",
    "    name = re.sub(\"\\(.*?\\)\", \" \", name)\n",
    "    res = m.analyze(name)\n",
    "    fem, mal = 0, 0\n",
    "    if len(res) == 6:\n",
    "        res = res[2:]\n",
    "    for i in res:\n",
    "        if 'analysis' in i:\n",
    "            try:\n",
    "                if ',жен,' in i['analysis'][0]['gr']:\n",
    "                    fem += 1\n",
    "                elif ',муж,' in i['analysis'][0]['gr']:\n",
    "                    mal += 1\n",
    "            except:\n",
    "                pass\n",
    "    if fem > 0 and mal == 0:\n",
    "        return 'f'\n",
    "    elif fem == 0 and mal > 0:\n",
    "        return 'm'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_inform_df(origin: pd.DataFrame, place_df: pd.DataFrame):\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"old_id\", \"code\", \"name\", \"gender\", \"birth_year\", \\\n",
    "        \"bio\", \"current_region\", \"current_district\", \\\n",
    "        \"current_village\", \"birth_region\", \"birth_district\", \\\n",
    "        \"birth_village\", \"current_geo_id\", \"birth_geo_id\"]\n",
    "    )\n",
    "    df[[\"code\", \"name\", \"bio\"]] = origin[[\"инициалы\", \"ФИО\", \"биография\"]]\n",
    "\n",
    "    df[[\n",
    "        \"current_village\",\n",
    "        \"current_region\",\n",
    "        \"current_district\",\n",
    "        \"birth_village\",\n",
    "        \"birth_region\",\n",
    "        \"birth_district\"\n",
    "    ]] = pd.DataFrame(origin[\"село\"]).merge(\n",
    "        place_df, left_on=[\"село\"], right_on=[\"village_name\"], how=\"left\"\n",
    "    )[[\n",
    "        \"village_name\",\n",
    "        \"map_region\",\n",
    "        \"map_district\",\n",
    "        \"village_name\",\n",
    "        \"map_region\",\n",
    "        \"map_district\"\n",
    "    ]]\n",
    "\n",
    "    df[\"gender\"] = origin[\"ФИО\"].apply(g_name)\n",
    "    df[\"birth_year\"] = origin[\"биография\"].apply(get_year)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_collector_df(origin: pd.DataFrame):\n",
    "    df = pd.DataFrame(columns=[\"old_id\", \"code\", \"name\"])\n",
    "    df[[\"old_id\", \"code\", \"name\"]] = origin[[\"Номер\", \"Код собирателя\", \"ФИО собирателя\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_text_pivot(origin: pd.DataFrame, text_table: JoinedTable, stub:str):\n",
    "    \"aggregates columns собиратель% or информант%\"\n",
    "\n",
    "    ids = text_table.new_indices\n",
    "    assert ids.shape[0] > 0\n",
    "    \n",
    "    target_cols = [i for i in origin.columns if stub in i]\n",
    "    subset: pd.DataFrame = origin[target_cols]\n",
    "    subset.insert(0, column=\"id\", value=ids)\n",
    "\n",
    "    long = pd.melt(subset, id_vars=[\"id\"], value_vars=target_cols)\n",
    "    long.dropna(axis=0, inplace=True)\n",
    "    long.drop([\"variable\"], axis=1, inplace=True)\n",
    "\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_list_pivot(main_file: pd.DataFrame, text_table: JoinedTable, key: str=\"ключевые слова\"):\n",
    "    ids = text_table.new_indices\n",
    "    assert ids.shape[0] > 0\n",
    "    observations = []\n",
    "    for idx, word_list in zip(ids, main_file[key]):\n",
    "        if not isinstance(word_list, str):\n",
    "            continue\n",
    "        word_list = word_list.split(\", \")\n",
    "        observations += [{\"id\": idx, \"value\": word} for word in word_list]\n",
    "    return pd.DataFrame.from_records(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_questions(x):\n",
    "    orig = x\n",
    "    x = re.sub(\"([0-9]+)([а-яa-z])\", \"\\g<1> \\g<2>\", x)\n",
    "    result = defaultdict(list)\n",
    "    row = wordpunct_tokenize(x)\n",
    "    cur_num = 0\n",
    "    for r in row:\n",
    "        if r.isdigit():\n",
    "            if cur_num not in result and cur_num != 0:\n",
    "                result[cur_num].append(\"\")\n",
    "            cur_num = int(r)\n",
    "        elif r in {\",\", \".\", \";\", \".,\"}:\n",
    "            continue\n",
    "        elif r.isalpha():\n",
    "            if r not in result[cur_num]:\n",
    "                result[cur_num].append(r)\n",
    "            elif r == \"доп\":\n",
    "                result[0].append(r)\n",
    "        else:\n",
    "            print(r)\n",
    "    if cur_num not in result and cur_num != 0:\n",
    "        result[cur_num].append(\"\")\n",
    "        \n",
    "    final = []\n",
    "    for q in result:\n",
    "        for subq in result[q]:\n",
    "            final.append(\n",
    "                str(q) + (\" \" + subq if subq else \"\")\n",
    "            )\n",
    "    return sorted(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_question_pivot(main_file: pd.DataFrame, text_table: JoinedTable):\n",
    "    df = main_file[[\"программа\", \"вопрос\"]].apply(\n",
    "        lambda x: list(\n",
    "            map(lambda y: x[\"программа\"] + \"-\" + y, parse_questions(x[\"вопрос\"]))\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    observations = []\n",
    "    ids = text_table.new_indices\n",
    "    for idx, q_list in zip(ids, df):\n",
    "        observations += [{\"id\":idx, \"value\":n} for n in q_list]\n",
    "    return pd.DataFrame.from_records(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_question_cols(x):\n",
    "    return [\n",
    "        x[\"id\"],\n",
    "        (\n",
    "            x[\"question_list\"] \n",
    "            + \"-\" + str(x[\"question_num\"]) \n",
    "            + ((\" \" + x[\"question_letter\"]) if x[\"question_letter\"] else \"\")\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_media_df(main_file: pd.DataFrame, text_table: JoinedTable, keys=(\"zvuk\", \"audio\")):\n",
    "    df = pd.DataFrame(columns=[\"id_text\", keys[1], \"start\"])\n",
    "    df[\"id_text\"] = text_table.new_indices\n",
    "    df[keys[1]] = main_file[keys[0]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "# names = FileNames(main=\"data/undone/novolokti.xlsx\")\n",
    "def main(names: FileNames, geonames: GeoNames=None):\n",
    "    \"\"\":param names: Paths of files that will be inserted into the dataframe\"\"\"\n",
    "    engine = sqlalchemy.create_engine(f\"mysql+pymysql://{c.USER}:{c.PASSWORD}@{c.HOST}:{c.PORT}/{c.DB}\")\n",
    "    main_file = read_main_file(names.main)\n",
    "    inform_file = read_sup_file(names.informators)\n",
    "    collector_file = read_sup_file(names.collectors)\n",
    "    text_df = produce_text_df(main_file)\n",
    "\n",
    "    # geo operations\n",
    "    villages = JoinedTable(tablename=\"g_villages\", engine=engine)\n",
    "    regions = JoinedTable(tablename=\"g_regions\", engine=engine)\n",
    "    districts = JoinedTable(tablename=\"g_districts\", engine=engine)\n",
    "    place_df, region_df, district_df = produce_region_district(\n",
    "        (\n",
    "            produce_place_df(main_file, inform_file, geonames=geonames) if geonames \n",
    "            else produce_place_df_complex(main_file, inform_file)\n",
    "        )\n",
    "    )\n",
    "    villages.join(place_df).drop_duplicates(\n",
    "        subset=[\"village_name\", \"map_region\", \"map_district\"]\n",
    "    ).upload()\n",
    "    (regions\n",
    "    .join(region_df)\n",
    "    # .drop_duplicates()\n",
    "    .drop_duplicates(inplace=True)\n",
    "    )\n",
    "    regions.upload()\n",
    "    (districts\n",
    "    .join(district_df)\n",
    "    # .drop_duplicates()\n",
    "    .drop_duplicates(inplace=True)\n",
    "    )\n",
    "    districts.upload()\n",
    "    #\n",
    "    geo_df = produce_geo_df(\n",
    "        villages.new_entries,\n",
    "        regions.dataframe,\n",
    "        districts.dataframe\n",
    "    )\n",
    "    geo_table = JoinedTable(\"g_geo_text\", engine=engine)\n",
    "    geo_table.join(geo_df)\n",
    "    text_df[\"geo_id\"] = get_geo_ids(\n",
    "        main_file,\n",
    "        villages.new_entries,\n",
    "        geo_table.new_entries\n",
    "    )\n",
    "    # print(text_df.tail())\n",
    "    # raise Exception\n",
    "    geo_table.dataframe = geo_table.dataframe.set_index(\"id\").drop(\n",
    "        list(set(geo_table.new_indices) - set(text_df[\"geo_id\"])),\n",
    "        axis=0\n",
    "    ).reset_index()\n",
    "    geo_table.upload()\n",
    "    #\n",
    "    # text operations\n",
    "    texts = JoinedTable(tablename=\"texts\", engine=engine)\n",
    "    texts.join(text_df)\n",
    "    texts.upload()\n",
    "    # inform operations\n",
    "    inform_df = produce_inform_df(inform_file, place_df)\n",
    "    inf_table = JoinedTable(\"informators\", engine)\n",
    "    inf_table.join(inform_df).upload()\n",
    "    inf_pivot = produce_text_pivot(main_file, texts, \"информант\")\n",
    "    t_i = JoinedRelation(\"t_i\", engine)\n",
    "    t_i.join(\n",
    "        inf_pivot,\n",
    "        inf_table.new_entries,\n",
    "        suffixes=(\"_text\", \"_informator\")\n",
    "    ).upload()    \n",
    "    # collector operations\n",
    "    collector_table = JoinedTable(\"collectors\", engine)\n",
    "    collector_df = produce_collector_df(collector_file)\n",
    "    collector_table.join(collector_df)\n",
    "    collector_table.dataframe.drop_duplicates(subset=[\"code\", \"name\"], inplace=True)\n",
    "    collector_table.upload()\n",
    "    col_pivot = produce_text_pivot(main_file, texts, \"собиратель\")\n",
    "    t_c = JoinedRelation(\"t_c\", engine)\n",
    "    t_c.join(\n",
    "        col_pivot,\n",
    "        collector_table.dataframe,\n",
    "        suffixes=(\"_text\", \"_collector\")\n",
    "    ).upload()\n",
    "    #keyword operations\n",
    "    keyword_df = produce_list_pivot(main_file, texts)\n",
    "    keyword_table = JoinedTable(\"keywords\", engine)\n",
    "    t_k = JoinedRelation(\"t_k\", engine)\n",
    "    t_k.join(\n",
    "        keyword_df,\n",
    "        keyword_table.dataframe,\n",
    "        on=(\"value\", \"word\"),\n",
    "        suffixes=(\"_text\", \"_keyword\")\n",
    "    ).upload()\n",
    "    # question operations\n",
    "    question_pivot = produce_question_pivot(main_file, texts)\n",
    "    questions = JoinedTable(\"questions\", engine)\n",
    "    quest_alias = questions.dataframe.apply(\n",
    "        concat_question_cols,\n",
    "        axis=1,\n",
    "        result_type='expand'\n",
    "    )\n",
    "    quest_alias.columns = [\"id\", \"question\"]\n",
    "    t_q = JoinedRelation(\"t_q\", engine)\n",
    "    t_q.join(\n",
    "        question_pivot,\n",
    "        quest_alias,\n",
    "        on=(\"value\", \"question\"),\n",
    "        suffixes=(\"_text\", \"_question\")\n",
    "    ).upload()\n",
    "    audio_table = JoinedTable(\"t_audio\", engine)\n",
    "    audio_df = produce_media_df(main_file, texts).dropna(axis=0)\n",
    "    audio_table.join(audio_df).upload()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    FileNames(\n",
    "        \"data/undone/tblCards.csv\",\n",
    "        \"data/undone/tblInformators.csv\",\n",
    "        \"data/undone/tblSobirately.csv\"\n",
    "    ),\n",
    "    # GeoNames(district=\"Чериковская область\", region=\"Могилевский район\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    names = sys.argv[1:]\n",
    "    for name in names:\n",
    "        assert os.path.isfile(name)\n",
    "    \n",
    "    main(names)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(\"mysql+pymysql://root:qwerty@localhost:3306/folklore_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# ids = pd.read_csv(\"data/ids.csv\", sep=\",\", quotechar='\\\\', index_col=None)\n",
    "# qs = pd.read_csv(\"data/qs.csv\", sep=\",\", quotechar='\\\\', index_col=None)\n",
    "# ids['text'] = ids['text'].apply(lambda x: x[:50])\n",
    "# qs['text'] = qs['text'].apply(lambda x: x[:50])\n",
    "# new = pd.merge(left=ids, right=qs, how='inner', left_on=[\"text\"], right_on=['text'])[[\"id\", \"prog\", \"вопрос\"]]\n",
    "# new.columns = [\"id\", 'prog', 'quest']\n",
    "# df = new[[\"prog\", \"quest\"]].apply(\n",
    "#     lambda x: list(\n",
    "#         map(lambda y: x[\"prog\"] + \"-\" + y, parse_questions(x[\"quest\"]))\n",
    "#     ),\n",
    "#     axis=1\n",
    "# )\n",
    "# observations = []\n",
    "# ids = new.id\n",
    "# for idx, q_list in zip(ids, df):\n",
    "#     observations += [{\"id\":idx, \"value\":n} for n in q_list]\n",
    "# q_mapping = pd.DataFrame.from_records(observations)\n",
    "\n",
    "# questions = JoinedTable(\"questions\", engine)\n",
    "# quest_alias = questions.dataframe.apply(\n",
    "#     concat_question_cols,\n",
    "#     axis=1,\n",
    "#     result_type='expand'\n",
    "# )\n",
    "# quest_alias.columns = [\"id\", \"question\"]\n",
    "# t_q = JoinedRelation(\"t_q\", engine)\n",
    "# t_q.join(\n",
    "#     q_mapping,\n",
    "#     quest_alias,\n",
    "#     on=(\"value\", \"question\"),\n",
    "#     suffixes=(\"_text\", \"_question\")\n",
    "# )\n",
    "# t_q.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_file = read_main_file(\"data/undone/tblCards.csv\",)\n",
    "# keyword_table = JoinedTable(\"keywords\", engine)\n",
    "# texts = JoinedTable(tablename=\"texts\", engine=engine)\n",
    "# texts_other = texts.dataframe.copy()\n",
    "# texts_other.raw_text = texts_other.raw_text.dropna().apply(lambda x: x[:65])\n",
    "# main_file = main_file[[\"текст\", \"ключевые слова\"]]\n",
    "# main_file.columns = [\"text\", \"keywords\"]\n",
    "# main_file.text = main_file.text.apply(lambda x: x[:65])\n",
    "# newkw = pd.merge(left=texts_other, right=main_file, left_on=['raw_text'], right_on=[\"text\"], how='inner')\n",
    "# newkw.head(5)\n",
    "# ids = newkw.id\n",
    "# assert ids.shape[0] > 0\n",
    "# observations = []\n",
    "# for idx, word_list in zip(ids, newkw.keywords):\n",
    "#     if not isinstance(word_list, str):\n",
    "#         continue\n",
    "#     word_list = word_list.split(\", \")\n",
    "#     observations += [{\"id\": idx, \"value\": word} for word in word_list]\n",
    "# kws = pd.DataFrame.from_records(observations)\n",
    "# t_k = JoinedRelation(\"t_k\", engine)\n",
    "# t_k.join(\n",
    "#     kws,\n",
    "#     keyword_table.dataframe,\n",
    "#     on=(\"value\", \"word\"),\n",
    "#     suffixes=(\"_text\", \"_keyword\")\n",
    "# )\n",
    "# t_k.upload()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
